{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as a reference, documenting additional details about testing data and preparatory steps for our two experiments. Processed data, which can be directly used for the experiments as demo-ed the Evaluation notebook, are also provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from convokit import Corpus, Utterance, Speaker, download\n",
    "from convokit import TextParser, PolitenessStrategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two types of experiments, each illustrating one potential source of misalignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data for experiment A (translated communication) is saved as `mt-test-corpus`, which consists of sampled test utterances involving the use of any of the four politeness strategies: {_Subjunctive_, _Please_, _Filler_, _Swearing_} which tend to be lost often when translating from English to Chinese at the time when we obtained the translations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_corpus = Corpus(filename=\"data/test/mt-test-corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes the following utterance-level metadata:\n",
    "\n",
    "    - strategy: strategy the utterance is sampled from. Should be one of {Subjunctive, Please, Filler, Swearing}\n",
    "    - parsed: parses for the utterance \n",
    "    - back-translation: back-translation for the original text, i.e., output after having the utterance go through English to Chinese translation and then translate back from Chinese to English. \n",
    "    \n",
    "An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy': 'Please',\n",
       " 'parsed': [{'rt': 3,\n",
       "   'toks': [{'tok': 'can', 'tag': 'MD', 'dep': 'aux', 'up': 3, 'dn': []},\n",
       "    {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 3, 'dn': []},\n",
       "    {'tok': 'please', 'tag': 'UH', 'dep': 'intj', 'up': 3, 'dn': []},\n",
       "    {'tok': 'remove', 'tag': 'VB', 'dep': 'ROOT', 'dn': [0, 1, 2, 5, 14]},\n",
       "    {'tok': 'the', 'tag': 'DT', 'dep': 'det', 'up': 5, 'dn': []},\n",
       "    {'tok': 'ability', 'tag': 'NN', 'dep': 'dobj', 'up': 3, 'dn': [4, 7]},\n",
       "    {'tok': 'to', 'tag': 'TO', 'dep': 'aux', 'up': 7, 'dn': []},\n",
       "    {'tok': 'edit', 'tag': 'VB', 'dep': 'acl', 'up': 5, 'dn': [6, 10, 11]},\n",
       "    {'tok': 'helio', 'tag': 'NNP', 'dep': 'poss', 'up': 10, 'dn': [9]},\n",
       "    {'tok': \"'s\", 'tag': 'POS', 'dep': 'case', 'up': 8, 'dn': []},\n",
       "    {'tok': 'page', 'tag': 'NN', 'dep': 'dobj', 'up': 7, 'dn': [8]},\n",
       "    {'tok': 'by', 'tag': 'IN', 'dep': 'prep', 'up': 7, 'dn': [13]},\n",
       "    {'tok': 'anonymous', 'tag': 'JJ', 'dep': 'amod', 'up': 13, 'dn': []},\n",
       "    {'tok': 'users', 'tag': 'NNS', 'dep': 'pobj', 'up': 11, 'dn': [12]},\n",
       "    {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 3, 'dn': []}]},\n",
       "  {'rt': 1,\n",
       "   'toks': [{'tok': 'its', 'tag': 'PRP$', 'dep': 'nsubj', 'up': 1, 'dn': []},\n",
       "    {'tok': 'getting', 'tag': 'VBG', 'dep': 'ROOT', 'dn': [0, 4, 5]},\n",
       "    {'tok': 'a', 'tag': 'DT', 'dep': 'det', 'up': 3, 'dn': []},\n",
       "    {'tok': 'bit', 'tag': 'NN', 'dep': 'npadvmod', 'up': 4, 'dn': [2]},\n",
       "    {'tok': 'ridiculous', 'tag': 'JJ', 'dep': 'acomp', 'up': 1, 'dn': [3]},\n",
       "    {'tok': '.', 'tag': '.', 'dep': 'punct', 'up': 1, 'dn': []}]}],\n",
       " 'back_translation': \"Can you remove the ability of anonymous users to edit daylight pages? It's become a bit ridiculous.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_corpus.get_utterance('226408211.416.416').meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the four strategies identified as high-risk do have relatively low retention rate, by aggregating strategies in the back-translations, and those in the original corpuses, and compute the % of loss. \n",
    "\n",
    "PS: note that while we aim to situate our experiments in realisitic settings, our focus is not to rigorously analyze how to best estimate politeness signals that are lost. As such, we take the reasonable approach of using back-tranlsation as proxies for what might be left or lost after translation to set up the experiments, but leave more robust and rigourous assessment for MT systems in maintaining politeness signals to future work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PolitenessStrategies(strategy_attribute_name = \"strategies\", \\\n",
    "                          marker_attribute_name = \"markers\", \\\n",
    "                          strategy_collection=\"politeness_local\")\n",
    "\n",
    "spacy_nlp = spacy.load('en', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get politeness strategies for the back translations \n",
    "translated_strategies = {utt.id: ps.transform_utterance(utt.meta['back_translation'], \\\n",
    "                         spacy_nlp=spacy_nlp, markers=True) for utt in mt_corpus.iter_utterances()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Strategy Preservation Rate===\n",
      "SUBJUNCTIVE: 20.0% \n",
      "PLEASE: 20.0% \n",
      "FILLER: 12.5% \n",
      "SWEARING: 37.0% \n"
     ]
    }
   ],
   "source": [
    "cnts = {'Subjunctive': 0, \"Please\": 0, 'Filler': 0, \"Swearing\": 0}\n",
    "\n",
    "for utt in mt_corpus.iter_utterances():\n",
    "    \n",
    "    k = utt.meta['strategy']\n",
    "    cnts[k] += translated_strategies[utt.id].meta['strategies'][k]\n",
    "\n",
    "print(\"===Strategy Preservation Rate===\")\n",
    "for k, v in cnts.items():\n",
    "    \n",
    "    # counts are out of a total of 200 instances  \n",
    "    print(\"{}: {:.1f}% \".format(k.upper(), v/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment B\n",
    "\n",
    "Test data for experiment B (misaligned perceptions) is saved as `ind-test-corpus`, which contains sampled test utterances that are predicted to result in larger differences in perceived politeness levels between pairs of annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_corpus = Corpus(filename=\"data/test/ind-test-corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes the following utterance-level metadata:\n",
    "\n",
    "    - sender: turker acting like the sender in this simulated setting\n",
    "    - receiver: turker acting like the receiver in this simulated settimg\n",
    "    - original_id: id of the utterance in WikiConv\n",
    "    - parsed: parses for the utterance \n",
    "        \n",
    "See the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sender': 'A23',\n",
       " 'receiver': 'A3S',\n",
       " 'original_id': '398481738.1386.1371',\n",
       " 'parsed': [{'rt': 1,\n",
       "   'toks': [{'tok': 'I', 'tag': 'PRP', 'dep': 'nsubj', 'up': 1, 'dn': []},\n",
       "    {'tok': 'added', 'tag': 'VBD', 'dep': 'ROOT', 'dn': [0, 3, 9]},\n",
       "    {'tok': 'one', 'tag': 'CD', 'dep': 'nummod', 'up': 3, 'dn': []},\n",
       "    {'tok': 'sentence', 'tag': 'NN', 'dep': 'dobj', 'up': 1, 'dn': [2, 4]},\n",
       "    {'tok': 'about', 'tag': 'IN', 'dep': 'prep', 'up': 3, 'dn': [6]},\n",
       "    {'tok': 'her', 'tag': 'PRP$', 'dep': 'poss', 'up': 6, 'dn': []},\n",
       "    {'tok': 'construction',\n",
       "     'tag': 'NN',\n",
       "     'dep': 'pobj',\n",
       "     'up': 4,\n",
       "     'dn': [5, 7, 8]},\n",
       "    {'tok': 'and', 'tag': 'CC', 'dep': 'cc', 'up': 6, 'dn': []},\n",
       "    {'tok': 'design', 'tag': 'NN', 'dep': 'conj', 'up': 6, 'dn': []},\n",
       "    {'tok': '.', 'tag': '.', 'dep': 'punct', 'up': 1, 'dn': []}]},\n",
       "  {'rt': 4,\n",
       "   'toks': [{'tok': 'What', 'tag': 'WP', 'dep': 'dobj', 'up': 4, 'dn': [1]},\n",
       "    {'tok': 'else', 'tag': 'RB', 'dep': 'advmod', 'up': 0, 'dn': []},\n",
       "    {'tok': 'would', 'tag': 'MD', 'dep': 'aux', 'up': 4, 'dn': []},\n",
       "    {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 4, 'dn': []},\n",
       "    {'tok': 'suggest', 'tag': 'VB', 'dep': 'ROOT', 'dn': [0, 2, 3, 5]},\n",
       "    {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 4, 'dn': []}]}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_corpus.get_utterance(\"A23-A3S-1\").meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider 20 different communication setting, as specified by the _(sender, receiver)_ pair. We consider 100 examples for each pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(utt.meta['sender'], utt.meta['receiver']) for utt in ind_corpus.iter_utterances()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('A23', 'A2U'): 100,\n",
       "         ('A23', 'A1F'): 100,\n",
       "         ('A23', 'A3S'): 100,\n",
       "         ('A23', 'AYG'): 100,\n",
       "         ('A2U', 'A23'): 100,\n",
       "         ('A2U', 'A1F'): 100,\n",
       "         ('A2U', 'A3S'): 100,\n",
       "         ('A2U', 'AYG'): 100,\n",
       "         ('A1F', 'A23'): 100,\n",
       "         ('A1F', 'A2U'): 100,\n",
       "         ('A1F', 'A3S'): 100,\n",
       "         ('A1F', 'AYG'): 100,\n",
       "         ('A3S', 'A23'): 100,\n",
       "         ('A3S', 'A2U'): 100,\n",
       "         ('A3S', 'A1F'): 100,\n",
       "         ('A3S', 'AYG'): 100,\n",
       "         ('AYG', 'A23'): 100,\n",
       "         ('AYG', 'A2U'): 100,\n",
       "         ('AYG', 'A1F'): 100,\n",
       "         ('AYG', 'A3S'): 100})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perception models \n",
    "\n",
    "The sender's and receiver's perceptions models are part of the circumstance we aim to incorporate. To set up the experiments. To model how an individual may perceive politeness, we use linear regression models as approximations. \n",
    "\n",
    "For Experiment A, we consider an _average person_, by taking the average scores from annotations from the Wikipedia portion of the Stanford Politeness Corpus. For experiment B, we consider the top 5 most prolific annotators from the Stanford Politeness Corpus to learn their perception models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from perception_utils import scale_func, get_strategies_df, get_model_info, get_ind_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Average-person model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /kitchen/convokit_corpora_lf/wikipedia-politeness-corpus\n"
     ]
    }
   ],
   "source": [
    "wiki_politeness = Corpus(download(\"wikipedia-politeness-corpus\"))\n",
    "\n",
    "wiki_politeness = ps.transform(wiki_politeness, markers = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for interpretability, we re-scale the annotations to -3 to 3 range \n",
    "for utt in wiki_politeness.iter_utterances():\n",
    "    \n",
    "    utt.meta['avg_score'] = scale_func(np.mean(list(utt.meta['Annotations'].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = get_strategies_df(wiki_politeness, 'strategies')\n",
    "scores = [wiki_politeness.get_utterance(idx).meta['avg_score'] for idx in df_avg.index]\n",
    "\n",
    "avg_model = get_model_info(df_avg, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefs are saved in `/perception_models/average.json`. We can get a feel for the strength of each strategy by looking at the coefficients: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coefs': {'Actually': -0.35832310780678395,\n",
       "  'Adverb.Just': -0.003919148991569088,\n",
       "  'Affirmation': 0.17106202210309893,\n",
       "  'Apology': 0.4294026792755307,\n",
       "  'By.The.Way': 0.33073420981106394,\n",
       "  'Conj.Start': -0.2449076077632873,\n",
       "  'Filler': -0.24450549480703548,\n",
       "  'For.Me': 0.12754640242716142,\n",
       "  'For.You': 0.19676631448406204,\n",
       "  'Gratitude': 0.9891183440757589,\n",
       "  'Greeting': 0.49055689805230834,\n",
       "  'Hedges': 0.1310668053348257,\n",
       "  'Indicative': 0.22110648465666793,\n",
       "  'Please': 0.22987732522609228,\n",
       "  'Please.Start': -0.2088696526965267,\n",
       "  'Reassurance': 0.6682643217184243,\n",
       "  'Subjunctive': 0.4540683006967976,\n",
       "  'Swearing': -1.2995687206430528},\n",
       " 'intercept': 0.10544430163739996}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Individualized models for prolific annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the top 5 annotators from the Wikipedia portion of the Stanford Politeness Corpus. They will be referred to by the first 3 characters of their id. Their annotations are saved as a ConvoKit corpus, with the following utterance level metadata:\n",
    "\n",
    "- score: scaled score provided by the 'speaker' (i.e., annotator) for the given utterance\n",
    "- original id: id of the utterance as it appears in the original dataset\n",
    "- parsed: parses for the utterance text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "turker_corpus = Corpus(filename=\"data/perceptions/turker-corpus/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of them has provided over 500 annotations, with the most prolific annotators having done 2,063 utterances: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'A2U': 1430, 'A23': 2063, 'AYG': 715, 'A3S': 832, 'A1F': 962})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([utt.speaker.id for utt in turker_corpus.iter_utterances()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract politeness markers and train individualized perception models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "turker_corpus = ps.transform(turker_corpus, markers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkers = ['A23', 'A2U', 'A1F', 'A3S', 'AYG']\n",
    "turker_dfs = defaultdict()\n",
    "\n",
    "for turker in turkers:\n",
    "    \n",
    "    corpus = turker_corpus.filter_utterances_by(selector=lambda utt:utt.speaker.id == turker)\n",
    "    \n",
    "    df = get_strategies_df(corpus, \"strategies\")\n",
    "    \n",
    "    df['score'] = [corpus.get_utterance(idx).meta['score'] for idx in df.index]\n",
    "    \n",
    "    turker_dfs[turker] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then obtain individual perception models as follows (the resultant models are saved in `/perception_models/<turker_prefix>.json`, but detailed model information is also printed below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A23\n",
      "{'coefs': {'Actually': -0.39958752122564184, 'Adverb.Just': -0.09733664110682594, 'Affirmation': -0.10983282152085155, 'Apology': -0.1980578971909959, 'By.The.Way': 0.36428388231865705, 'Conj.Start': -0.22488431102767364, 'Gratitude': 0.6285864971752283, 'Greeting': 0.4557020205871193, 'Hedges': 0.18643497711609897, 'Indicative': 0.7326680955961387, 'Please': 0.3962552437581173, 'Please.Start': -0.20488489664695417, 'Subjunctive': 1.1019655188010744, 'Filler': -0.24450549480703548, 'For.Me': 0.12754640242716142, 'Reassurance': 0.6682643217184243, 'Swearing': -1.2995687206430528, 'For.You': 0.19676631448406204}, 'intercept': -0.07388039486261758}\n",
      "======\n",
      "A2U\n",
      "{'coefs': {'Actually': -0.2832263741915894, 'Adverb.Just': 0.06926703482339108, 'Affirmation': -0.08968564618061908, 'Apology': 0.36252514772196825, 'By.The.Way': 0.3271172242009981, 'Conj.Start': -0.12103532461519026, 'Gratitude': 0.9641442769677369, 'Greeting': 0.34658127900946856, 'Hedges': 0.19394608324520143, 'Indicative': 0.16329447927825197, 'Please': 0.3801929220364721, 'Please.Start': -0.1868875831331429, 'Subjunctive': 0.2975215733648756, 'Filler': -0.24450549480703548, 'For.Me': 0.12754640242716142, 'Reassurance': 0.6682643217184243, 'Swearing': -1.2995687206430528, 'For.You': 0.19676631448406204}, 'intercept': 0.32713184568108655}\n",
      "======\n",
      "A1F\n",
      "{'coefs': {'Actually': -0.4272443503562189, 'Adverb.Just': 0.008969758848357712, 'Affirmation': 0.48478717304624414, 'Apology': 0.47129288976565037, 'By.The.Way': 0.7032017609770927, 'Conj.Start': -0.09700679270404167, 'Gratitude': 0.5794167043260272, 'Greeting': 0.4682560299764772, 'Hedges': 0.09333359790661835, 'Indicative': 0.2461908116483416, 'Please': -0.18208376297448495, 'Subjunctive': 0.517375581736105, 'Filler': -0.24450549480703548, 'For.Me': 0.12754640242716142, 'Reassurance': 0.6682643217184243, 'Swearing': -1.2995687206430528, 'For.You': 0.19676631448406204, 'Please.Start': -0.2088696526965267}, 'intercept': -0.0029510787266832927}\n",
      "======\n",
      "A3S\n",
      "{'coefs': {'Actually': -0.1809345450007311, 'Adverb.Just': -0.051587278531202485, 'Affirmation': 0.01962061248775564, 'Apology': 0.4618242279026127, 'Conj.Start': -0.10555462458372519, 'Gratitude': 1.3646460091928307, 'Greeting': 0.2772513609007291, 'Hedges': 0.020597458973574928, 'Indicative': 0.13268608379963656, 'Please': 0.6782324248021615, 'Subjunctive': 0.1725591271773133, 'Filler': -0.24450549480703548, 'For.Me': 0.12754640242716142, 'Reassurance': 0.6682643217184243, 'Swearing': -1.2995687206430528, 'By.The.Way': 0.33073420981106394, 'For.You': 0.19676631448406204, 'Please.Start': -0.2088696526965267}, 'intercept': -0.1404853624051975}\n",
      "======\n",
      "AYG\n",
      "{'coefs': {'Actually': -0.24260355807084202, 'Adverb.Just': 0.017113133159772198, 'Affirmation': -0.06894141673246988, 'Apology': 0.3438126820519547, 'Conj.Start': -0.17020460034979856, 'Gratitude': 0.5595735024070261, 'Greeting': 0.21183959892892693, 'Hedges': 0.0827343044676478, 'Indicative': 0.11599040968729207, 'Please': 0.23782766734875077, 'Subjunctive': 0.12683499041477697, 'Filler': -0.24450549480703548, 'For.Me': 0.12754640242716142, 'Reassurance': 0.6682643217184243, 'Swearing': -1.2995687206430528, 'By.The.Way': 0.33073420981106394, 'For.You': 0.19676631448406204, 'Please.Start': -0.2088696526965267}, 'intercept': 0.24786170211228503}\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "avg_coefs = avg_model['coefs']\n",
    "\n",
    "for t, df in turker_dfs.items():\n",
    "            \n",
    "    df_feat = df.iloc[:, 0:-1]\n",
    "    scores = dict(df.iloc[:, -1])\n",
    "\n",
    "    ind_model = get_ind_model_info(df_feat, scores, avg_coefs)     \n",
    "  \n",
    "    print(t)\n",
    "    print(ind_model)\n",
    "    print('======')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
