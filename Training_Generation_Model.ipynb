{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the generation model\n",
    "\n",
    "In order to teach the generation model to incorporate strategies, we need to show examples of how each strategy can be integrated into various utterance contexts. \n",
    "\n",
    "To this end, we sample a set of utterances as _groundtruth_ data, and then seperate the _strategy_ used in utterances from the remaining utterance _context_ to create (_strategy_, _context_, _groundtrugh_) tuples as training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from convokit import Corpus, Utterance, Speaker\n",
    "from convokit import PolitenessStrategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training data\n",
    "\n",
    "Training data for the generation model is sampled from [WikiConv](http://www.cs.cornell.edu/~cristian/index_files/wikiconv-conversation-corpus.pdf). For\n",
    "each politeness strategy, we sample 1,500 disjoint instances. \n",
    "\n",
    "The training data is saved as a [ConvoKit](https://convokit.cornell.edu/) corpus, which includes the following utterance-level metadata:\n",
    "        \n",
    "        * strategy: strategy split\n",
    "        * parsed: dependency parse information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = Corpus(filename=(\"data/train/training-corpus/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Seperating strategy markers and utterance context\n",
    "\n",
    "We first identify strategies (and their corresponding markers) used in the utterances. For data directly read from the TSV file, the same step can be performed with transform_utterance on raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PolitenessStrategies(strategy_attribute_name = \"strategies\", \\\n",
    "                          marker_attribute_name = \"markers\", \\\n",
    "                          strategy_collection=\"politeness_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is important to set markers to True\n",
    "train_corpus = ps.transform(train_corpus, markers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that each utterance does indeed contain the markers for strategies they are representing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for utt in train_corpus.iter_utterances():\n",
    "    \n",
    "    strategy_split = utt.meta['strategy']\n",
    "    assert utt.meta['strategies'][strategy_split] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each (_strategy_, _utterance_) pair---i.e., (_utt.meta['strategy']_, _utt.text_) for each utt in our corpus)---we obtain the version of utterance with the specified strategy removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions further detailed in Marker_Edits.ipynb \n",
    "from strategy_manipulation import remove_strategies_from_utt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for utt in train_corpus.iter_utterances():\n",
    "    \n",
    "    remove_strategies_from_utt(utt, [utt.meta['strategy']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each utterance in the corpus now has the strategy-removed content saved under _post_del_content_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE: Is this page really still a stub?  Seems like enough information to remove the stub marker.\n",
      "AFTER: is this page still a stub ?   seems like enough information to remove the stub marker .\n"
     ]
    }
   ],
   "source": [
    "utt = train_corpus.get_utterance('100087711.41.31')\n",
    "print(\"BEFORE:\", utt.text)\n",
    "print(\"AFTER:\", utt.meta['post_del_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare generation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from strategy_manipulation import convert_to_training_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format for training data is as follows (not that special tokens are introduced as seperators): \n",
    "\n",
    "    <STR> strategy_name <CONTEXT> content <START> groundtruth <END>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for utt in train_corpus.iter_utterances():\n",
    "    \n",
    "    strategy = utt.meta['strategy']\n",
    "    post_del_content = utt.meta['post_del_content']\n",
    "    text = utt.text.lower()\n",
    "    \n",
    "    utt.meta['training_format'] = convert_to_training_format(strategy, \\\n",
    "                                                             post_del_content, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "\n",
    "train_data = [utt.meta['training_format'] for utt in train_corpus.iter_utterances() \\\n",
    "                      if utt.meta['split'] == \"train\"]\n",
    "\n",
    "eval_data = [utt.meta['training_format'] for utt in train_corpus.iter_utterances() \\\n",
    "                      if utt.meta['split'] == \"eval\"]\n",
    "\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can specify where you prefer the formatted outputs are written to\n",
    "# the resultant files can also be directly found in data/training-files\n",
    "out_dir = \"generation_data/data/\"\n",
    "\n",
    "formatted_data = {\"train\": train_data, \"eval\": eval_data}\n",
    "\n",
    "for filetype in [\"train\", \"eval\"]:\n",
    "    \n",
    "    with open(os.path.join(out_dir, \"{}.txt\".format(filetype)), \"w+\") as f:\n",
    "        for data in formatted_data[filetype]:\n",
    "            f.write(\"{}\\n\".format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training \n",
    "\n",
    "The training script ([openai_gpt_drg_strategized.py](openai_gpt_drg_strategized.py)) is adapted from the training script [openai_gpt_delete_retrive_and_generate.py](https://github.com/agaralabs/transformer-drg-style-transfer/blob/master/openai_gpt_delete_retrive_and_generate.py) from [transformer-drg-style-transfer](https://github.com/agaralabs/transformer-drg-style-transfer), with the key change being an update to the special tokens used. \n",
    "\n",
    "We use the following setting to train the generation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python openai_gpt_drg_strategized.py --do_train --do_eval \\\n",
    "      --train_dataset \"data/train/training-files/train.txt\" \\\n",
    "      --eval_dataset \"data/train/training-files/eval.txt\" \\\n",
    "      --train_batch_size 8 \\\n",
    "      --eval_batch_size 8 \\\n",
    "      --max_seq_length 128 \\\n",
    "      --output_dir \"politeness_paraphrase/models\" \\\n",
    "      --num_train_epochs 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
